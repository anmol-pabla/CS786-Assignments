{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "190154_Memory",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>CS786 Assignment-3 on Memory Models by Anmol Pabla (190154)<h1>\n",
        "(Converted to Python)"
      ],
      "metadata": {
        "id": "-qxUWLCuQHDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1: Filling in the partially complete code to make a TCM"
      ],
      "metadata": {
        "id": "HoIB80wijqHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Temporal Context Model (TCM) takes into consideration the effect of the environment. It assumes a linear drift of the temporal context cue. The temporal context model assumes that the past becomes increasingly dissimilar to the future, so that memories become harder to retrieve the farther away in the past they are.\n",
        "\n",
        "---\n",
        "\n",
        "Here, we are going to model the world as a set of N continuous-valued features. We will model observations of states of the world as samples from N Gaussians with time-varying means and fixed variance. For simplicity, assume that agents change nothing in the world."
      ],
      "metadata": {
        "id": "tp4PqkGrRBnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing necessary files\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "2mjOV7VrVOGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function provided in Matlab converted to python\n",
        "def drawFromADist(p):\n",
        "  if(np.sum(p)==0):\n",
        "    p = 0.05*np.ones((1,len(p)))  \n",
        "  p = p/np.sum(p)\n",
        "  c = np.cumsum(p)\n",
        "  # Choosing the minimum index that satisfies below condition\n",
        "  idx = np.where((random.random() - np.cumsum(p))<0)\n",
        "  sample = np.min(idx)\n",
        "  out = np.zeros(p.size)\n",
        "  # Index represented by sample is the value retrieved in this case\n",
        "  out[sample] = 1\n",
        "  return out"
      ],
      "metadata": {
        "id": "VtmD48LclKay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNsxW6vmU8E0"
      },
      "outputs": [],
      "source": [
        "def TCM(N_WORLD_FEATURES, N_ITEMS, ENCODING_TIME, TEST_TIME):\n",
        "\n",
        "  # first fix the presentation schedule; Here its random \n",
        "  schedule = np.transpose([np.sort(np.round_(np.random.random(N_ITEMS)*ENCODING_TIME)), np.arange(0,N_ITEMS)])\n",
        "  schedule_load = ENCODING_TIME/np.median(np.diff(schedule[:,1]))    \n",
        "\n",
        "  # variable that acts as the proxy learning load on the agent\n",
        "  encoding = np.zeros((N_ITEMS,N_WORLD_FEATURES))\n",
        "\n",
        "  # initial means\n",
        "  world_m = [1,2,1,2,1]\n",
        "\n",
        "  # initial variance\n",
        "  world_var = 1\n",
        "  delta = 0.05                       # causes linear drift in means\n",
        "  beta_param = 0.001                 # used in the next part of the assignmnet\n",
        "  m = 0\n",
        "\n",
        "\n",
        "  for time in range(ENCODING_TIME):\n",
        "      world_m = [k + delta for k in world_m]\n",
        "      world = random.gauss(world_m, np.sqrt(world_var))\n",
        "      #  items encoded in memory, in association with the\n",
        "      #  state of the world at that time.\n",
        "      if m<N_ITEMS:\n",
        "          if(time==schedule[m,1]):\n",
        "              encoding[m] = world                                                  \n",
        "              # encode into the encoding vector\n",
        "              m =  m + 1\n",
        "\n",
        "  # simulating retrieval using SAM, but with a bijective image-item mapping\n",
        "\n",
        "  out = np.empty(TEST_TIME)\n",
        "  soa = np.empty(N_ITEMS)\n",
        "  time = ENCODING_TIME\n",
        "\n",
        "  while (time < ENCODING_TIME+TEST_TIME):\n",
        "    # the state of the world is the retrieval cue\n",
        "    world_m = [k + delta for k in world_m]\n",
        "    world = random.gauss(world_m, np.sqrt(world_var))\n",
        "\n",
        "    # model world evolution\n",
        "    for m in range(0,N_ITEMS):\n",
        "        soa[m] = np.dot(world,encoding[m])                                                                \n",
        "        # finding association strengths\n",
        "\n",
        "    out[time-ENCODING_TIME] = np.where(drawFromADist(soa))[0]\n",
        "    time = time + 1\n",
        "\n",
        "  success = (np.unique(out)).size                                     \n",
        "  # success is number of unique retrievals\n",
        "  return success\n",
        "\n",
        "  #  humans can retrieve about 7 items effectively from memory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_success = 0\n",
        "\n",
        "# averaging success over 10 cycles\n",
        "for i in range(10):\n",
        "  curr_success = TCM(N_WORLD_FEATURES = 5, N_ITEMS = 10, ENCODING_TIME = 500, TEST_TIME = 20)\n",
        "  avg_success += curr_success\n",
        "  \n",
        "avg_success = avg_success/10\n",
        "\n",
        "print('Average Sucess: ',avg_success)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c-GwgNe39g5",
        "outputId": "1f09cfe8-bea0-4bc3-fac6-1b30da117000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Sucess:  7.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We find that the model manages to maintain an average success rate which is above 7, the value that is generally expected in humans. Thus, it can be said that the model works reasonably well.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3tSnxrd20oGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2: Sampling the rate of drift from a mixture of two Gaussians"
      ],
      "metadata": {
        "id": "wWpHPYtRjwJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the Gaussians has a smaller mean (about 0.05) and the other has an mean of about 2. Both the Gaussians have a variance of 1.\n",
        "\n",
        "The values are chosen from this Gaussian mixture based on the beta parameter, values from the smaller meaned Gaussian are chosen with a probability of beta_param."
      ],
      "metadata": {
        "id": "ngtZWZN11rUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def TCM_2(N_WORLD_FEATURES, N_ITEMS, ENCODING_TIME, TEST_TIME, TIMES):\n",
        "\n",
        "  # first fix the presentation schedule passed as the argument TIMES in this case\n",
        "\n",
        "  schedule = np.transpose([TIMES, np.arange(0,N_ITEMS)])\n",
        "  schedule_load = ENCODING_TIME/np.median(np.diff(schedule[:,0]))          \n",
        "\n",
        "  # variable proxy for learning load\n",
        "  encoding = np.zeros((N_ITEMS,N_WORLD_FEATURES))\n",
        "\n",
        "  # Gaussian Means\n",
        "  world_m = [1,2,1,2,1]\n",
        "\n",
        "  world_var = 1\n",
        "  # Averages: One bigger and one smaller\n",
        "  delta_avg_large = 2                      \n",
        "  delta_avg_small = 0.05\n",
        "  # Parameter that defines probability of choosing smaller average from the bimodal mixture\n",
        "  beta_param = 0.001                \n",
        "  \n",
        "  m = 0\n",
        "  for time in range(ENCODING_TIME):\n",
        "    d1 = random.gauss(delta_avg_large, np.sqrt(world_var))\n",
        "    d2 = random.gauss(delta_avg_small, np.sqrt(world_var))\n",
        "    # Choosing value from one of the Gaussian distributions based on beta_param\n",
        "    delta = random.choices([d1,d2],weights=[beta_param,1-beta_param]) \n",
        "\n",
        "    world_m = [k + delta[0] for k in world_m]\n",
        "    world = random.gauss(world_m, np.sqrt(world_var))\n",
        "\n",
        "    if m<N_ITEMS:\n",
        "        if(time==schedule[m,1]):\n",
        "            encoding[m] = world                                                  \n",
        "            # encode into the encoding vector\n",
        "            m =  m + 1\n",
        "\n",
        "  # simulating retrieval using SAM, but with a bijective image-item mapping\n",
        "  out = np.empty(TEST_TIME)\n",
        "  soa = np.empty(N_ITEMS)\n",
        "\n",
        "  time = ENCODING_TIME\n",
        "  while (time < ENCODING_TIME+TEST_TIME):\n",
        "    # the state of the world is the retrieval cue\n",
        "    # Modelling the drift from the Gaussians (Parameters known in this case)\n",
        "    d1 = random.gauss(delta_avg_large, np.sqrt(world_var))\n",
        "    d2 = random.gauss(delta_avg_small, np.sqrt(world_var))\n",
        "    delta = random.choices([d1,d2],weights=[beta_param,1-beta_param]) \n",
        "\n",
        "    world_m = [k + delta[0] for k in world_m]\n",
        "    world =  random.gauss(world_m, np.sqrt(world_var))\n",
        "\n",
        "    # model world evolution\n",
        "    for m in range(0,N_ITEMS):\n",
        "        soa[m] = np.dot(world,encoding[m])                                                                \n",
        "        # finding association strengths\n",
        "\n",
        "    out[time-ENCODING_TIME] = np.where(drawFromADist(soa))[0]\n",
        "    time = time + 1\n",
        "\n",
        "  success = (np.unique(out)).size                                     \n",
        "  # success is number of unique retrievals\n",
        "  return success, schedule_load\n",
        "\n",
        "  #  humans can retrieve about 7 items effectively from memory. "
      ],
      "metadata": {
        "id": "lmR-Z-RJjxfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Now we look at the retrieval rates for different Encoding Schedules:\n"
      ],
      "metadata": {
        "id": "jd7rW1mG3BmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we fix the presentaion schedule at the end of the encoding time."
      ],
      "metadata": {
        "id": "nxdoqcGjC7Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Presentaion schedule shoved to the end\n",
        "END_TIME = [491,492,493,494,495,496,497,498,499,500]\n",
        "\n",
        "avg_success = 0\n",
        "avg_load = 0\n",
        "# averaging over 10 values\n",
        "for i in range(10):\n",
        "  curr_success, curr_load = TCM_2(N_WORLD_FEATURES = 5, N_ITEMS = 10, ENCODING_TIME = 500, TEST_TIME = 20, TIMES=END_TIME)\n",
        "  avg_success += curr_success\n",
        "  avg_load += curr_load\n",
        "\n",
        "avg_success = avg_success/10\n",
        "avg_load = avg_load/10\n",
        "\n",
        "print('Average Sucess: ',avg_success)\n",
        "print('Average Load: ',avg_load)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyW8vD9Zqwr0",
        "outputId": "ea3a4785-5b2c-4edd-e707-ceea5e384a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Sucess:  7.6\n",
            "Average Load:  500.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the above case has a good Success rate but significantly high learning load (Maximum value of the load is obtained in the above case)."
      ],
      "metadata": {
        "id": "hfFmcEWOZQUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "The minimum load occurs when the median value is as high as possible, this is seen when 5 items are scheduled at the end and the others are spaced with the biggest margin possible (about 100 units of time in this case)\n"
      ],
      "metadata": {
        "id": "sHv3NhcJZl3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_LOAD_TIME = [0,99,198,297,396,496,497,498,499,500]\n",
        "avg_success = 0\n",
        "avg_load = 0\n",
        "\n",
        "for i in range(10):\n",
        "  curr_success, curr_load = TCM_2(N_WORLD_FEATURES = 5, N_ITEMS = 10, ENCODING_TIME = 500, TEST_TIME = 20, TIMES=MIN_LOAD_TIME)\n",
        "  avg_success += curr_success\n",
        "  avg_load += curr_load\n",
        "\n",
        "avg_success = avg_success/10\n",
        "avg_load = avg_load/10\n",
        "\n",
        "print('Average Sucess: ',avg_success)\n",
        "print('Average Load: ',avg_load)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfvXDsgutJr1",
        "outputId": "281defc6-bd7d-42b9-f339-c80a8078000b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Sucess:  7.4\n",
            "Average Load:  5.050505050505051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We find that the load is reduced to a minimum and the success rate is also decent. However, it is quite clear that we are exploiting the way the load is calculated and half of the presentations are still fixed at the end, and in real human scenarios this method would not lead to the minimum possible learning load. \n",
        "\n",
        "Acheiving a realisitic solution would possibly require modelling all events with an equal amount of time between them. This is what one expects to observe in the real world situation as well.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Yn3-PB7JaQOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MID_TIME = []\n",
        "step = 500/10\n",
        "time = 50\n",
        "\n",
        "# times are equally spaced, the last value being at 500\n",
        "while time<=500:\n",
        "  MID_TIME.append(int(time))\n",
        "  time = time + step\n",
        "  \n",
        "avg_success = 0\n",
        "avg_load = 0 \n",
        "\n",
        "# averaging over 10 values\n",
        "for i in range(10):\n",
        "  curr_success, curr_load = TCM_2(N_WORLD_FEATURES = 5, N_ITEMS = 10, ENCODING_TIME = 500, TEST_TIME = 20, TIMES=MID_TIME)\n",
        "  avg_success += curr_success\n",
        "  avg_load += curr_load\n",
        "\n",
        "avg_success = avg_success/10\n",
        "avg_load = avg_load/10\n",
        "\n",
        "print('Average Sucess: ',avg_success)\n",
        "print('Average Load: ',avg_load)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTdDPqKcuKy4",
        "outputId": "803a39d2-cd1a-46a5-8c3f-8c26655d7e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Sucess:  7.2\n",
            "Average Load:  10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We seem to have achieved a reasonable position in our TCM with the load not being too high but managing to maintain a retrieval rate of about 7 items which is generally seen in humans.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Hn5TColqa4db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3: Modelling a similar case as Q2, but the retrieving agent doesn't know parameters of the Bimodal Distribution used to sample the drift."
      ],
      "metadata": {
        "id": "HHPsH1a2Hw9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Here we need to use the EM algorithm to learn the parameters of the bimodal distribution and allow the retrieval agent to use them. An expectation–maximization (EM) algorithm is an iterative method to find (local) maximum likelihood estimates of parameters in statistical models, where the model depends on unobserved latent variables. \n",
        "\n",
        "Here, we use the EM algorithm to learn the parameters of the mixed Gaussian distribution. The algorithm is an iterative algorithm that starts from some initial estimates of the parameters, and then proceeds to\n",
        "iteratively update the values until convergence is detected. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-u-g58Fg5Mvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In this model, the EM algorithm is implemented using the GaussianMixture function from the Scikit-Learn Library. \n",
        "\n",
        "The function takes a distribution as an argument and infers Gaussians present in that distribution. The function is designed to detect n_components number of Gaussians which is also passed as an argument."
      ],
      "metadata": {
        "id": "TFDSoiks5fqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# Function to learn the Gaussians in the Bimodal Distribution using the EM algorithm\n",
        "# Return a lst of samples generated from the Gaussian mixture\n",
        "\n",
        "def get_samples_from_EM(deltas):\n",
        "  deltas = np.array(deltas)\n",
        "  deltas = deltas.reshape(-1, 1)\n",
        "  # Model learns using the distribution given as input\n",
        "  gm = GaussianMixture(n_components=2, random_state=0).fit(deltas)\n",
        "  # sampling 100 values using the learned parameters\n",
        "  delta_samples = gm.sample(1000)\n",
        "  delta_samples = delta_samples[0].reshape(-1)\n",
        "  return delta_samples  "
      ],
      "metadata": {
        "id": "d-W6GCc5Tka7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TCM_3(N_WORLD_FEATURES, N_ITEMS, ENCODING_TIME, TEST_TIME, TIMES):\n",
        "\n",
        "  # first fix the presentation schedule passed using TIMES\n",
        "\n",
        "  schedule = np.transpose([TIMES, np.arange(0,N_ITEMS)])\n",
        "  schedule_load = ENCODING_TIME/np.median(np.diff(schedule[:,0]))                 \n",
        "  # variable proxy for learning load\n",
        "  encoding = np.zeros((N_ITEMS,N_WORLD_FEATURES))\n",
        "\n",
        "  world_m = [1,2,1,2,1]\n",
        "\n",
        "  world_var = 1\n",
        "  # small and large mean for Gaussians\n",
        "  delta_avg_large = 2                      \n",
        "  delta_avg_small = 0.05\n",
        "  # beta_param to choose from the distrbution\n",
        "  beta_param = 0.001                 \n",
        "  m = 0\n",
        "\n",
        "  # list to save the values of drift obtained using Ecnoding\n",
        "  delta_data = []\n",
        "\n",
        "  for time in range(ENCODING_TIME):\n",
        "    d1 = random.gauss(delta_avg_large, np.sqrt(world_var))\n",
        "    d2 = random.gauss(delta_avg_small, np.sqrt(world_var))\n",
        "    # Choosing value from one of the Gaussian distributions based on beta_param\n",
        "    delta = random.choices([d1,d2],weights=[beta_param,1-beta_param]) \n",
        "    delta_data.append(delta)\n",
        "\n",
        "    world_m = [k + delta[0] for k in world_m]\n",
        "    world = random.gauss(world_m, np.sqrt(world_var))\n",
        "    #  Items encoed in association with the state of the world at that time.\n",
        "    if m<N_ITEMS:\n",
        "        if(time==schedule[m,1]):\n",
        "            encoding[m] = world                                                  \n",
        "            # encode into the encoding vector\n",
        "            m =  m + 1\n",
        "\n",
        "  # simulating retrieval using SAM, but with a bijective image-item mapping\n",
        "  out = np.empty(TEST_TIME)\n",
        "  soa = np.empty(N_ITEMS)\n",
        "\n",
        "  # Learning the parameters of the Gaussian mixture and\n",
        "  # getting list of values sampled from the distribution, \n",
        "  # these will be used for obtaining the drift \n",
        "  delta_samples = get_samples_from_EM(delta_data)\n",
        "\n",
        "  time = ENCODING_TIME\n",
        "  while (time < ENCODING_TIME+TEST_TIME):\n",
        "    # the state of the world is the retrieval cue\n",
        "    # randomly choosing the value of drift from the samples obtained above\n",
        "    delta = random.choice(delta_samples) \n",
        "\n",
        "    world_m = [k + delta for k in world_m]\n",
        "    world =  random.gauss(world_m, np.sqrt(world_var))\n",
        "    # model world evolution\n",
        "    for m in range(0,N_ITEMS):\n",
        "        soa[m] = np.dot(world,encoding[m])                                                                \n",
        "        # finding association strengths\n",
        "    out[time-ENCODING_TIME] = np.where(drawFromADist(soa))[0]\n",
        "    time = time + 1\n",
        "\n",
        "  success = (np.unique(out)).size                                     \n",
        "  # success is number of unique retrievals\n",
        "  return success, schedule_load\n",
        "\n",
        "  #  humans can retrieve about 7 items effectively from memory. get this model\n",
        "  #  to behave like humans"
      ],
      "metadata": {
        "id": "0Us3KVR-H7JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like in the previous Question, we use three Encoding schedules:\n",
        "\n",
        "\n",
        "*   Shoving Presentations to the end of the encoding schedule, this leads to a high retrieval rate but also a significantly high load.\n",
        "*   Maximizing the median gap in presentation schedule to minimize the Load.\n",
        "*   Distibuting Presentations uniformly to try and get an optimal but realsitc Load and retrieval.\n",
        "\n"
      ],
      "metadata": {
        "id": "3xDKHlbP-b-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All presentations at the end of the schedule\n",
        "END_TIME = [491,492,493,494,495,496,497,498,499,500]\n",
        "\n",
        "avg_success = 0\n",
        "avg_load = 0\n",
        "for i in range(10):\n",
        "  curr_success, curr_load = TCM_3(N_WORLD_FEATURES = 5, N_ITEMS = 10, ENCODING_TIME = 500, TEST_TIME = 20, TIMES=END_TIME)\n",
        "  avg_success += curr_success\n",
        "  avg_load += curr_load\n",
        "\n",
        "avg_success = avg_success/10\n",
        "avg_load = avg_load/10\n",
        "\n",
        "print('Average Sucess: ',avg_success)\n",
        "print('Average Load: ',avg_load)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJlNkqFfTjQx",
        "outputId": "8dfa1f44-f847-4f2c-ce1d-b50f38550cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Sucess:  7.5\n",
            "Average Load:  500.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We find a high load and high retrieval.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "J4IXukceFTZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the minimum load time by maximizing the median time difference\n",
        "MIN_LOAD_TIME = [0,99,198,297,396,496,497,498,499,500]\n",
        "avg_success = 0\n",
        "avg_load = 0\n",
        "\n",
        "for i in range(10):\n",
        "  curr_success, curr_load = TCM_3(N_WORLD_FEATURES = 5, N_ITEMS = 10, ENCODING_TIME = 500, TEST_TIME = 20, TIMES=MIN_LOAD_TIME)\n",
        "  avg_success += curr_success\n",
        "  avg_load += curr_load\n",
        "\n",
        "avg_success = avg_success/10\n",
        "avg_load = avg_load/10\n",
        "\n",
        "print('Average Sucess: ',avg_success)\n",
        "print('Average Load: ',avg_load)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h79lnbFXPCG3",
        "outputId": "219cf19d-8620-46d9-f0be-9b1b38037a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Sucess:  7.3\n",
            "Average Load:  5.050505050505051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above case, the load is low but presentations are still towards the end. Next we move onto a realistic solution to the problem."
      ],
      "metadata": {
        "id": "a6zyDH10FbOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realistic case: Equally spaced presentation\n",
        "MID_TIME = []\n",
        "step = 500/10\n",
        "time = 50\n",
        "\n",
        "while time<=500:\n",
        "  MID_TIME.append(int(time))\n",
        "  time = time + step\n",
        "  \n",
        "avg_success = 0\n",
        "avg_load = 0 \n",
        "\n",
        "for i in range(10):\n",
        "  curr_success, curr_load = TCM_3(N_WORLD_FEATURES = 5, N_ITEMS = 10, ENCODING_TIME = 500, TEST_TIME = 20, TIMES=MID_TIME)\n",
        "  avg_success += curr_success\n",
        "  avg_load += curr_load\n",
        "\n",
        "avg_success = avg_success/10\n",
        "avg_load = avg_load/10\n",
        "\n",
        "print('Average Sucess: ',avg_success)\n",
        "print('Average Load: ',avg_load)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVcNTWn4PFjp",
        "outputId": "559dfba3-1e3b-4c0c-e79f-0b682e82f1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Sucess:  7.0\n",
            "Average Load:  10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We obtain a rate similar to human performance but with a decently low learning load.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3ERaF8ZQFudM"
      }
    }
  ]
}